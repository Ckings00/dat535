{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wont run our data on more than two nodes. Trying to reshuffle the data using hadoop to split the data across more files. Then to send it in to hadoop again hopefully such that it is split more evenly across the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"Ingestion\").getOrCreate())\n",
    "hadoop_rdd = spark.sparkContext.textFile(\"hdfs://namenode:9000/input\", minPartitions=120)\n",
    "hadoop_rdd = hadoop_rdd.map(lambda x: x.split(\",\"))\n",
    "df = hadoop_rdd.toDF()\n",
    "df.write.mode(\"overwrite\").csv(\"/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires pulling the data from hadoop and then sending it back in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
